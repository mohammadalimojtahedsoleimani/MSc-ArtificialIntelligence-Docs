{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIG45Z14DA0Q"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uctDUrp5ZGGe"
      },
      "source": [
        "# Mohammad Ali Mojtahed Soleimani"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XbU4lpUDA0S"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from PIL import Image\n",
        "import os\n",
        "import cv2\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGM8LeapDA0U"
      },
      "source": [
        "# Requirements for Part A.\n",
        "\n",
        "*   Load Hockey dataset.\n",
        "*   Import ResNet50.\n",
        "*   Remove the last FC layer of ResNet to use network as a feature extractor.\n",
        "*   Extract frames from videos.\n",
        "*   ImageNet normalization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kipxq7cLDA0U"
      },
      "outputs": [],
      "source": [
        "class HockeyDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, num_frames=16):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (string): Directory with all the videos categorized into subfolders.\n",
        "            transform (callable, optional): Optional transform to be applied on each frame.\n",
        "            num_frames (int): Number of frames to extract from each video.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.num_frames = num_frames\n",
        "        self.classes = sorted(os.listdir(root_dir))\n",
        "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
        "        self.samples = self._make_dataset(self.root_dir, self.class_to_idx)\n",
        "\n",
        "    def _make_dataset(self, root_dir, class_to_idx):\n",
        "        videos = []\n",
        "        for target_class in sorted(class_to_idx.keys()):\n",
        "            class_index = class_to_idx[target_class]\n",
        "            target_dir = os.path.join(root_dir, target_class)\n",
        "            if not os.path.isdir(target_dir):\n",
        "                continue\n",
        "            for root, _, fnames in sorted(os.walk(target_dir, followlinks=True)):\n",
        "                for fname in sorted(fnames):\n",
        "                    if fname.endswith(('.avi', '.mp4', '.mov')):  \n",
        "                        path = os.path.join(root, fname)\n",
        "                        item = (path, class_index)\n",
        "                        videos.append(item)\n",
        "        return videos\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path, label = self.samples[idx]\n",
        "        frames = self.extract_frames(video_path)\n",
        "\n",
        "        \n",
        "        if self.transform:\n",
        "            frames_tensor = torch.stack([self.transform(frame) for frame in frames])\n",
        "        else:\n",
        "            frames_tensor = torch.stack([transforms.ToTensor()(frame) for frame in frames])\n",
        "\n",
        "        return frames_tensor, label\n",
        "\n",
        "    def extract_frames(self, video_path):\n",
        "        \"\"\"\n",
        "        Extracts frames from a video.\n",
        "        \"\"\"\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "        if total_frames < self.num_frames:\n",
        "            frame_indices = range(total_frames)\n",
        "        else:\n",
        "            frame_indices = torch.linspace(0, total_frames - 1, self.num_frames).long()\n",
        "\n",
        "        frames = []\n",
        "        for i in range(total_frames):\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            if i in frame_indices:\n",
        "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                frame = Image.fromarray(frame)\n",
        "                frames.append(frame)\n",
        "        cap.release()\n",
        "        return frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9T5R_t-ExbE"
      },
      "outputs": [],
      "source": [
        "def unzip_dataset(zip_file_path, extract_path):\n",
        "   \n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(f\"Dataset extracted to: {extract_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysGzfhXHDA0V"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ks6YRYyDA0V"
      },
      "outputs": [],
      "source": [
        "data_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # ResNet50 input size\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # ImageNet normalization\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjPr7AmUDA0W"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1ISJPPlDA0W",
        "outputId": "1ca48fd4-7dfb-436e-a2b1-4ca03026157e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset extracted to: extracted_data\n"
          ]
        }
      ],
      "source": [
        "\n",
        "zip_file_path = \"/content/Hockey.zip\"\n",
        "extract_path = \"extracted_data\"\n",
        "unzip_dataset(zip_file_path, extract_path)\n",
        "root_directory = extract_path\n",
        "hockey_dataset = HockeyDataset(root_dir=root_directory, transform=data_transform, num_frames=16)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_m9MTUjQXeaZ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5FYrluEXhd2"
      },
      "outputs": [],
      "source": [
        "num_samples = len(hockey_dataset)\n",
        "train_size = num_samples // 2  \n",
        "test_size = num_samples - train_size  # The rest for testing\n",
        "train_dataset, test_dataset = random_split(hockey_dataset, [train_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8pJaHcdDA0W"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFiumEFlDA0X",
        "outputId": "46b2ce31-f8e2-496b-f044-f862d6f8d08e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 181MB/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (2): ReLU(inplace=True)\n",
              "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (5): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (6): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (7): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              ")"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "resnet50 = models.resnet50(pretrained=True)\n",
        "\n",
        "feature_extractor = nn.Sequential(*list(resnet50.children())[:-1])\n",
        "\n",
        "feature_extractor.eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiWNyKzhDA0X"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ueiNNqXBDA0X"
      },
      "outputs": [],
      "source": [
        "def extract_features(data_loader, feature_extractor):\n",
        "    features_list = []\n",
        "    labels_list = []\n",
        "    with torch.no_grad():\n",
        "        for video_frames, labels in data_loader:\n",
        "            batch_size, num_frames, C, H, W = video_frames.shape\n",
        "            video_frames = video_frames.view(batch_size * num_frames, C, H, W)\n",
        "            features = feature_extractor(video_frames)\n",
        "            features = features.view(batch_size, num_frames, -1)\n",
        "            video_features = torch.mean(features, dim=1) # Average pooling\n",
        "            features_list.append(video_features)\n",
        "            labels_list.append(labels)\n",
        "    all_features = torch.cat(features_list, dim=0)\n",
        "    all_labels = torch.cat(labels_list, dim=0)\n",
        "    return all_features, all_labels\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JlQIF9tDA0Y"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "je0A5r9yDA0Y",
        "outputId": "c90d6747-d35c-49b4-f70b-2b0575c9b955"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train features shape: torch.Size([500, 2048])\n",
            "Train labels shape: torch.Size([500])\n",
            "Test features shape: torch.Size([500, 2048])\n",
            "Test labels shape: torch.Size([500])\n"
          ]
        }
      ],
      "source": [
        "train_features, train_labels = extract_features(train_loader, feature_extractor)\n",
        "test_features, test_labels = extract_features(test_loader, feature_extractor)\n",
        "\n",
        "print(\"Train features shape:\", train_features.shape)\n",
        "print(\"Train labels shape:\", train_labels.shape)\n",
        "print(\"Test features shape:\", test_features.shape)\n",
        "print(\"Test labels shape:\", test_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQiMiE-Thlwe"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OU1rSQowhmyZ"
      },
      "outputs": [],
      "source": [
        "data_to_save = {\n",
        "    'train_features': train_features,\n",
        "    'train_labels': train_labels,\n",
        "    'test_features': test_features,\n",
        "    'test_labels': test_labels\n",
        "}\n",
        "\n",
        "torch.save(data_to_save, 'extracted_data.pth')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fivcu1TDYI0j"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqN9t3kLYJI0"
      },
      "outputs": [],
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsvhsRmEYPv2"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKb-vLEEYQHK"
      },
      "outputs": [],
      "source": [
        "# --- Hyperparameters ---\n",
        "\n",
        "input_size = 2048  \n",
        "hidden_size = 512 \n",
        "num_classes = len(hockey_dataset.classes) \n",
        "learning_rate = 0.001\n",
        "num_epochs = 10  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GG6p3wguYVRq"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eRTTygLYVj7"
      },
      "outputs": [],
      "source": [
        "classifier = Classifier(input_size, hidden_size, num_classes)\n",
        "criterion = nn.CrossEntropyLoss()  \n",
        "optimizer = torch.optim.Adam(classifier.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7QIR3vKYWnH"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwvHLf5ZYW9b",
        "outputId": "6b0d4dc9-2cf2-45ec-d85c-5063d27c87a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Step [10/63], Loss: 1.4286\n",
            "Epoch [1/10], Step [20/63], Loss: 0.2939\n",
            "Epoch [1/10], Step [30/63], Loss: 0.6655\n",
            "Epoch [1/10], Step [40/63], Loss: 0.1335\n",
            "Epoch [1/10], Step [50/63], Loss: 1.2249\n",
            "Epoch [1/10], Step [60/63], Loss: 2.0242\n",
            "Epoch [1/10], Step [70/63], Loss: 0.1738\n",
            "Epoch [1/10], Step [80/63], Loss: 0.0344\n",
            "Epoch [1/10], Step [90/63], Loss: 0.0097\n",
            "Epoch [1/10], Step [100/63], Loss: 0.2364\n",
            "Epoch [1/10], Step [110/63], Loss: 0.0727\n",
            "Epoch [1/10], Step [120/63], Loss: 0.0069\n",
            "Epoch [1/10], Step [130/63], Loss: 0.0782\n",
            "Epoch [1/10], Step [140/63], Loss: 0.7987\n",
            "Epoch [1/10], Step [150/63], Loss: 0.1245\n",
            "Epoch [1/10], Step [160/63], Loss: 0.1808\n",
            "Epoch [1/10], Step [170/63], Loss: 0.0624\n",
            "Epoch [1/10], Step [180/63], Loss: 0.7095\n",
            "Epoch [1/10], Step [190/63], Loss: 0.2364\n",
            "Epoch [1/10], Step [200/63], Loss: 0.0817\n",
            "Epoch [1/10], Step [210/63], Loss: 0.1796\n",
            "Epoch [1/10], Step [220/63], Loss: 0.8439\n",
            "Epoch [1/10], Step [230/63], Loss: 0.0121\n",
            "Epoch [1/10], Step [240/63], Loss: 0.0020\n",
            "Epoch [1/10], Step [250/63], Loss: 0.0057\n",
            "Epoch [1/10], Step [260/63], Loss: 0.2607\n",
            "Epoch [1/10], Step [270/63], Loss: 2.8420\n",
            "Epoch [1/10], Step [280/63], Loss: 0.0024\n",
            "Epoch [1/10], Step [290/63], Loss: 0.0907\n",
            "Epoch [1/10], Step [300/63], Loss: 0.0061\n",
            "Epoch [1/10], Step [310/63], Loss: 0.0006\n",
            "Epoch [1/10], Step [320/63], Loss: 0.1711\n",
            "Epoch [1/10], Step [330/63], Loss: 1.8625\n",
            "Epoch [1/10], Step [340/63], Loss: 0.0393\n",
            "Epoch [1/10], Step [350/63], Loss: 0.0636\n",
            "Epoch [1/10], Step [360/63], Loss: 0.0669\n",
            "Epoch [1/10], Step [370/63], Loss: 0.0642\n",
            "Epoch [1/10], Step [380/63], Loss: 0.0465\n",
            "Epoch [1/10], Step [390/63], Loss: 0.0219\n",
            "Epoch [1/10], Step [400/63], Loss: 0.8466\n",
            "Epoch [1/10], Step [410/63], Loss: 0.0808\n",
            "Epoch [1/10], Step [420/63], Loss: 0.6970\n",
            "Epoch [1/10], Step [430/63], Loss: 0.0295\n",
            "Epoch [1/10], Step [440/63], Loss: 0.0326\n",
            "Epoch [1/10], Step [450/63], Loss: 0.1486\n",
            "Epoch [1/10], Step [460/63], Loss: 0.0222\n",
            "Epoch [1/10], Step [470/63], Loss: 0.7052\n",
            "Epoch [1/10], Step [480/63], Loss: 0.0106\n",
            "Epoch [1/10], Step [490/63], Loss: 0.3641\n",
            "Epoch [1/10], Step [500/63], Loss: 0.0217\n",
            "Epoch [2/10], Step [10/63], Loss: 0.8641\n",
            "Epoch [2/10], Step [20/63], Loss: 0.0099\n",
            "Epoch [2/10], Step [30/63], Loss: 0.0308\n",
            "Epoch [2/10], Step [40/63], Loss: 0.0002\n",
            "Epoch [2/10], Step [50/63], Loss: 0.0065\n",
            "Epoch [2/10], Step [60/63], Loss: 2.0604\n",
            "Epoch [2/10], Step [70/63], Loss: 0.0625\n",
            "Epoch [2/10], Step [80/63], Loss: 0.0018\n",
            "Epoch [2/10], Step [90/63], Loss: 0.1686\n",
            "Epoch [2/10], Step [100/63], Loss: 0.2158\n",
            "Epoch [2/10], Step [110/63], Loss: 0.0376\n",
            "Epoch [2/10], Step [120/63], Loss: 0.0170\n",
            "Epoch [2/10], Step [130/63], Loss: 0.4740\n",
            "Epoch [2/10], Step [140/63], Loss: 0.2545\n",
            "Epoch [2/10], Step [150/63], Loss: 0.1432\n",
            "Epoch [2/10], Step [160/63], Loss: 0.0741\n",
            "Epoch [2/10], Step [170/63], Loss: 0.0327\n",
            "Epoch [2/10], Step [180/63], Loss: 1.0269\n",
            "Epoch [2/10], Step [190/63], Loss: 0.1251\n",
            "Epoch [2/10], Step [200/63], Loss: 0.0515\n",
            "Epoch [2/10], Step [210/63], Loss: 0.0824\n",
            "Epoch [2/10], Step [220/63], Loss: 0.2048\n",
            "Epoch [2/10], Step [230/63], Loss: 0.0066\n",
            "Epoch [2/10], Step [240/63], Loss: 0.0010\n",
            "Epoch [2/10], Step [250/63], Loss: 0.0018\n",
            "Epoch [2/10], Step [260/63], Loss: 0.1737\n",
            "Epoch [2/10], Step [270/63], Loss: 1.8169\n",
            "Epoch [2/10], Step [280/63], Loss: 0.0105\n",
            "Epoch [2/10], Step [290/63], Loss: 0.1469\n",
            "Epoch [2/10], Step [300/63], Loss: 0.0144\n",
            "Epoch [2/10], Step [310/63], Loss: 0.0099\n",
            "Epoch [2/10], Step [320/63], Loss: 0.0502\n",
            "Epoch [2/10], Step [330/63], Loss: 0.3371\n",
            "Epoch [2/10], Step [340/63], Loss: 0.0028\n",
            "Epoch [2/10], Step [350/63], Loss: 0.0175\n",
            "Epoch [2/10], Step [360/63], Loss: 0.0412\n",
            "Epoch [2/10], Step [370/63], Loss: 0.0274\n",
            "Epoch [2/10], Step [380/63], Loss: 0.0286\n",
            "Epoch [2/10], Step [390/63], Loss: 0.0141\n",
            "Epoch [2/10], Step [400/63], Loss: 0.7563\n",
            "Epoch [2/10], Step [410/63], Loss: 0.0390\n",
            "Epoch [2/10], Step [420/63], Loss: 0.5916\n",
            "Epoch [2/10], Step [430/63], Loss: 0.0214\n",
            "Epoch [2/10], Step [440/63], Loss: 0.0206\n",
            "Epoch [2/10], Step [450/63], Loss: 0.0440\n",
            "Epoch [2/10], Step [460/63], Loss: 0.0305\n",
            "Epoch [2/10], Step [470/63], Loss: 0.3083\n",
            "Epoch [2/10], Step [480/63], Loss: 0.0020\n",
            "Epoch [2/10], Step [490/63], Loss: 0.3326\n",
            "Epoch [2/10], Step [500/63], Loss: 0.0188\n",
            "Epoch [3/10], Step [10/63], Loss: 0.4556\n",
            "Epoch [3/10], Step [20/63], Loss: 0.0051\n",
            "Epoch [3/10], Step [30/63], Loss: 0.0073\n",
            "Epoch [3/10], Step [40/63], Loss: 0.0000\n",
            "Epoch [3/10], Step [50/63], Loss: 0.0010\n",
            "Epoch [3/10], Step [60/63], Loss: 1.1966\n",
            "Epoch [3/10], Step [70/63], Loss: 0.0337\n",
            "Epoch [3/10], Step [80/63], Loss: 0.0007\n",
            "Epoch [3/10], Step [90/63], Loss: 0.1101\n",
            "Epoch [3/10], Step [100/63], Loss: 0.2077\n",
            "Epoch [3/10], Step [110/63], Loss: 0.0082\n",
            "Epoch [3/10], Step [120/63], Loss: 0.0255\n",
            "Epoch [3/10], Step [130/63], Loss: 0.2355\n",
            "Epoch [3/10], Step [140/63], Loss: 0.1123\n",
            "Epoch [3/10], Step [150/63], Loss: 0.0989\n",
            "Epoch [3/10], Step [160/63], Loss: 0.0467\n",
            "Epoch [3/10], Step [170/63], Loss: 0.0233\n",
            "Epoch [3/10], Step [180/63], Loss: 0.8189\n",
            "Epoch [3/10], Step [190/63], Loss: 0.1169\n",
            "Epoch [3/10], Step [200/63], Loss: 0.0218\n",
            "Epoch [3/10], Step [210/63], Loss: 0.0560\n",
            "Epoch [3/10], Step [220/63], Loss: 0.0788\n",
            "Epoch [3/10], Step [230/63], Loss: 0.0076\n",
            "Epoch [3/10], Step [240/63], Loss: 0.0008\n",
            "Epoch [3/10], Step [250/63], Loss: 0.0003\n",
            "Epoch [3/10], Step [260/63], Loss: 0.0260\n",
            "Epoch [3/10], Step [270/63], Loss: 0.3145\n",
            "Epoch [3/10], Step [280/63], Loss: 0.0321\n",
            "Epoch [3/10], Step [290/63], Loss: 0.1498\n",
            "Epoch [3/10], Step [300/63], Loss: 0.0135\n",
            "Epoch [3/10], Step [310/63], Loss: 0.0217\n",
            "Epoch [3/10], Step [320/63], Loss: 0.0088\n",
            "Epoch [3/10], Step [330/63], Loss: 0.1210\n",
            "Epoch [3/10], Step [340/63], Loss: 0.0006\n",
            "Epoch [3/10], Step [350/63], Loss: 0.0153\n",
            "Epoch [3/10], Step [360/63], Loss: 0.0229\n",
            "Epoch [3/10], Step [370/63], Loss: 0.0164\n",
            "Epoch [3/10], Step [380/63], Loss: 0.0238\n",
            "Epoch [3/10], Step [390/63], Loss: 0.0122\n",
            "Epoch [3/10], Step [400/63], Loss: 1.4044\n",
            "Epoch [3/10], Step [410/63], Loss: 0.0270\n",
            "Epoch [3/10], Step [420/63], Loss: 0.3556\n",
            "Epoch [3/10], Step [430/63], Loss: 0.0295\n",
            "Epoch [3/10], Step [440/63], Loss: 0.0192\n",
            "Epoch [3/10], Step [450/63], Loss: 0.0365\n",
            "Epoch [3/10], Step [460/63], Loss: 0.0425\n",
            "Epoch [3/10], Step [470/63], Loss: 0.2420\n",
            "Epoch [3/10], Step [480/63], Loss: 0.0011\n",
            "Epoch [3/10], Step [490/63], Loss: 0.3614\n",
            "Epoch [3/10], Step [500/63], Loss: 0.0160\n",
            "Epoch [4/10], Step [10/63], Loss: 0.1923\n",
            "Epoch [4/10], Step [20/63], Loss: 0.0026\n",
            "Epoch [4/10], Step [30/63], Loss: 0.0026\n",
            "Epoch [4/10], Step [40/63], Loss: 0.0000\n",
            "Epoch [4/10], Step [50/63], Loss: 0.0001\n",
            "Epoch [4/10], Step [60/63], Loss: 1.7419\n",
            "Epoch [4/10], Step [70/63], Loss: 0.0416\n",
            "Epoch [4/10], Step [80/63], Loss: 0.0004\n",
            "Epoch [4/10], Step [90/63], Loss: 0.0685\n",
            "Epoch [4/10], Step [100/63], Loss: 0.1378\n",
            "Epoch [4/10], Step [110/63], Loss: 0.0026\n",
            "Epoch [4/10], Step [120/63], Loss: 0.0335\n",
            "Epoch [4/10], Step [130/63], Loss: 0.0647\n",
            "Epoch [4/10], Step [140/63], Loss: 0.0466\n",
            "Epoch [4/10], Step [150/63], Loss: 0.0898\n",
            "Epoch [4/10], Step [160/63], Loss: 0.0429\n",
            "Epoch [4/10], Step [170/63], Loss: 0.0223\n",
            "Epoch [4/10], Step [180/63], Loss: 0.5543\n",
            "Epoch [4/10], Step [190/63], Loss: 0.0794\n",
            "Epoch [4/10], Step [200/63], Loss: 0.0062\n",
            "Epoch [4/10], Step [210/63], Loss: 0.0339\n",
            "Epoch [4/10], Step [220/63], Loss: 0.0367\n",
            "Epoch [4/10], Step [230/63], Loss: 0.0121\n",
            "Epoch [4/10], Step [240/63], Loss: 0.0011\n",
            "Epoch [4/10], Step [250/63], Loss: 0.0002\n",
            "Epoch [4/10], Step [260/63], Loss: 0.0119\n",
            "Epoch [4/10], Step [270/63], Loss: 0.0710\n",
            "Epoch [4/10], Step [280/63], Loss: 0.0533\n",
            "Epoch [4/10], Step [290/63], Loss: 0.0690\n",
            "Epoch [4/10], Step [300/63], Loss: 0.0138\n",
            "Epoch [4/10], Step [310/63], Loss: 0.0170\n",
            "Epoch [4/10], Step [320/63], Loss: 0.0005\n",
            "Epoch [4/10], Step [330/63], Loss: 0.5801\n",
            "Epoch [4/10], Step [340/63], Loss: 0.0011\n",
            "Epoch [4/10], Step [350/63], Loss: 0.0035\n",
            "Epoch [4/10], Step [360/63], Loss: 0.0045\n",
            "Epoch [4/10], Step [370/63], Loss: 0.0337\n",
            "Epoch [4/10], Step [380/63], Loss: 0.0152\n",
            "Epoch [4/10], Step [390/63], Loss: 0.0079\n",
            "Epoch [4/10], Step [400/63], Loss: 1.1834\n",
            "Epoch [4/10], Step [410/63], Loss: 0.0576\n",
            "Epoch [4/10], Step [420/63], Loss: 0.3550\n",
            "Epoch [4/10], Step [430/63], Loss: 0.0337\n",
            "Epoch [4/10], Step [440/63], Loss: 0.0125\n",
            "Epoch [4/10], Step [450/63], Loss: 0.0420\n",
            "Epoch [4/10], Step [460/63], Loss: 0.0592\n",
            "Epoch [4/10], Step [470/63], Loss: 0.2533\n",
            "Epoch [4/10], Step [480/63], Loss: 0.0007\n",
            "Epoch [4/10], Step [490/63], Loss: 0.2564\n",
            "Epoch [4/10], Step [500/63], Loss: 0.0205\n",
            "Epoch [5/10], Step [10/63], Loss: 0.2181\n",
            "Epoch [5/10], Step [20/63], Loss: 0.0022\n",
            "Epoch [5/10], Step [30/63], Loss: 0.0029\n",
            "Epoch [5/10], Step [40/63], Loss: 0.0000\n",
            "Epoch [5/10], Step [50/63], Loss: 0.0001\n",
            "Epoch [5/10], Step [60/63], Loss: 0.2584\n",
            "Epoch [5/10], Step [70/63], Loss: 0.0050\n",
            "Epoch [5/10], Step [80/63], Loss: 0.0003\n",
            "Epoch [5/10], Step [90/63], Loss: 0.0295\n",
            "Epoch [5/10], Step [100/63], Loss: 0.0586\n",
            "Epoch [5/10], Step [110/63], Loss: 0.0018\n",
            "Epoch [5/10], Step [120/63], Loss: 0.0281\n",
            "Epoch [5/10], Step [130/63], Loss: 0.0257\n",
            "Epoch [5/10], Step [140/63], Loss: 0.0265\n",
            "Epoch [5/10], Step [150/63], Loss: 0.0167\n",
            "Epoch [5/10], Step [160/63], Loss: 0.0357\n",
            "Epoch [5/10], Step [170/63], Loss: 0.0189\n",
            "Epoch [5/10], Step [180/63], Loss: 0.3480\n",
            "Epoch [5/10], Step [190/63], Loss: 0.0718\n",
            "Epoch [5/10], Step [200/63], Loss: 0.0021\n",
            "Epoch [5/10], Step [210/63], Loss: 0.0133\n",
            "Epoch [5/10], Step [220/63], Loss: 0.0192\n",
            "Epoch [5/10], Step [230/63], Loss: 0.0075\n",
            "Epoch [5/10], Step [240/63], Loss: 0.0005\n",
            "Epoch [5/10], Step [250/63], Loss: 0.0000\n",
            "Epoch [5/10], Step [260/63], Loss: 0.0045\n",
            "Epoch [5/10], Step [270/63], Loss: 0.0282\n",
            "Epoch [5/10], Step [280/63], Loss: 0.0863\n",
            "Epoch [5/10], Step [290/63], Loss: 0.0319\n",
            "Epoch [5/10], Step [300/63], Loss: 0.0208\n",
            "Epoch [5/10], Step [310/63], Loss: 0.0073\n",
            "Epoch [5/10], Step [320/63], Loss: 0.0003\n",
            "Epoch [5/10], Step [330/63], Loss: 0.7005\n",
            "Epoch [5/10], Step [340/63], Loss: 0.0011\n",
            "Epoch [5/10], Step [350/63], Loss: 0.0024\n",
            "Epoch [5/10], Step [360/63], Loss: 0.0021\n",
            "Epoch [5/10], Step [370/63], Loss: 0.0457\n",
            "Epoch [5/10], Step [380/63], Loss: 0.0133\n",
            "Epoch [5/10], Step [390/63], Loss: 0.0011\n",
            "Epoch [5/10], Step [400/63], Loss: 0.9897\n",
            "Epoch [5/10], Step [410/63], Loss: 0.0171\n",
            "Epoch [5/10], Step [420/63], Loss: 0.3412\n",
            "Epoch [5/10], Step [430/63], Loss: 0.0262\n",
            "Epoch [5/10], Step [440/63], Loss: 0.0037\n",
            "Epoch [5/10], Step [450/63], Loss: 0.0137\n",
            "Epoch [5/10], Step [460/63], Loss: 0.0679\n",
            "Epoch [5/10], Step [470/63], Loss: 0.1306\n",
            "Epoch [5/10], Step [480/63], Loss: 0.0001\n",
            "Epoch [5/10], Step [490/63], Loss: 0.3065\n",
            "Epoch [5/10], Step [500/63], Loss: 0.0144\n",
            "Epoch [6/10], Step [10/63], Loss: 0.1229\n",
            "Epoch [6/10], Step [20/63], Loss: 0.0012\n",
            "Epoch [6/10], Step [30/63], Loss: 0.0004\n",
            "Epoch [6/10], Step [40/63], Loss: 0.0000\n",
            "Epoch [6/10], Step [50/63], Loss: 0.0000\n",
            "Epoch [6/10], Step [60/63], Loss: 0.7119\n",
            "Epoch [6/10], Step [70/63], Loss: 0.0035\n",
            "Epoch [6/10], Step [80/63], Loss: 0.0001\n",
            "Epoch [6/10], Step [90/63], Loss: 0.0298\n",
            "Epoch [6/10], Step [100/63], Loss: 0.0742\n",
            "Epoch [6/10], Step [110/63], Loss: 0.0002\n",
            "Epoch [6/10], Step [120/63], Loss: 0.0378\n",
            "Epoch [6/10], Step [130/63], Loss: 0.0198\n",
            "Epoch [6/10], Step [140/63], Loss: 0.0207\n",
            "Epoch [6/10], Step [150/63], Loss: 0.0190\n",
            "Epoch [6/10], Step [160/63], Loss: 0.0179\n",
            "Epoch [6/10], Step [170/63], Loss: 0.0336\n",
            "Epoch [6/10], Step [180/63], Loss: 0.2953\n",
            "Epoch [6/10], Step [190/63], Loss: 0.0509\n",
            "Epoch [6/10], Step [200/63], Loss: 0.0007\n",
            "Epoch [6/10], Step [210/63], Loss: 0.0100\n",
            "Epoch [6/10], Step [220/63], Loss: 0.0130\n",
            "Epoch [6/10], Step [230/63], Loss: 0.0102\n",
            "Epoch [6/10], Step [240/63], Loss: 0.0006\n",
            "Epoch [6/10], Step [250/63], Loss: 0.0000\n",
            "Epoch [6/10], Step [260/63], Loss: 0.0023\n",
            "Epoch [6/10], Step [270/63], Loss: 0.0078\n",
            "Epoch [6/10], Step [280/63], Loss: 0.0626\n",
            "Epoch [6/10], Step [290/63], Loss: 0.0147\n",
            "Epoch [6/10], Step [300/63], Loss: 0.0162\n",
            "Epoch [6/10], Step [310/63], Loss: 0.0031\n",
            "Epoch [6/10], Step [320/63], Loss: 0.0002\n",
            "Epoch [6/10], Step [330/63], Loss: 0.4735\n",
            "Epoch [6/10], Step [340/63], Loss: 0.0004\n",
            "Epoch [6/10], Step [350/63], Loss: 0.0022\n",
            "Epoch [6/10], Step [360/63], Loss: 0.0016\n",
            "Epoch [6/10], Step [370/63], Loss: 0.0675\n",
            "Epoch [6/10], Step [380/63], Loss: 0.0032\n",
            "Epoch [6/10], Step [390/63], Loss: 0.0016\n",
            "Epoch [6/10], Step [400/63], Loss: 0.2982\n",
            "Epoch [6/10], Step [410/63], Loss: 0.0174\n",
            "Epoch [6/10], Step [420/63], Loss: 0.4155\n",
            "Epoch [6/10], Step [430/63], Loss: 0.0412\n",
            "Epoch [6/10], Step [440/63], Loss: 0.0018\n",
            "Epoch [6/10], Step [450/63], Loss: 0.0125\n",
            "Epoch [6/10], Step [460/63], Loss: 0.0703\n",
            "Epoch [6/10], Step [470/63], Loss: 0.1036\n",
            "Epoch [6/10], Step [480/63], Loss: 0.0000\n",
            "Epoch [6/10], Step [490/63], Loss: 0.1147\n",
            "Epoch [6/10], Step [500/63], Loss: 0.0062\n",
            "Epoch [7/10], Step [10/63], Loss: 0.0533\n",
            "Epoch [7/10], Step [20/63], Loss: 0.0019\n",
            "Epoch [7/10], Step [30/63], Loss: 0.0002\n",
            "Epoch [7/10], Step [40/63], Loss: 0.0000\n",
            "Epoch [7/10], Step [50/63], Loss: 0.0000\n",
            "Epoch [7/10], Step [60/63], Loss: 0.8810\n",
            "Epoch [7/10], Step [70/63], Loss: 0.0229\n",
            "Epoch [7/10], Step [80/63], Loss: 0.0003\n",
            "Epoch [7/10], Step [90/63], Loss: 0.0518\n",
            "Epoch [7/10], Step [100/63], Loss: 0.1645\n",
            "Epoch [7/10], Step [110/63], Loss: 0.0013\n",
            "Epoch [7/10], Step [120/63], Loss: 0.0187\n",
            "Epoch [7/10], Step [130/63], Loss: 0.0906\n",
            "Epoch [7/10], Step [140/63], Loss: 0.0377\n",
            "Epoch [7/10], Step [150/63], Loss: 0.0159\n",
            "Epoch [7/10], Step [160/63], Loss: 0.0098\n",
            "Epoch [7/10], Step [170/63], Loss: 0.0200\n",
            "Epoch [7/10], Step [180/63], Loss: 0.1697\n",
            "Epoch [7/10], Step [190/63], Loss: 0.0358\n",
            "Epoch [7/10], Step [200/63], Loss: 0.0005\n",
            "Epoch [7/10], Step [210/63], Loss: 0.0059\n",
            "Epoch [7/10], Step [220/63], Loss: 0.0159\n",
            "Epoch [7/10], Step [230/63], Loss: 0.0094\n",
            "Epoch [7/10], Step [240/63], Loss: 0.0008\n",
            "Epoch [7/10], Step [250/63], Loss: 0.0000\n",
            "Epoch [7/10], Step [260/63], Loss: 0.0021\n",
            "Epoch [7/10], Step [270/63], Loss: 0.0033\n",
            "Epoch [7/10], Step [280/63], Loss: 0.0617\n",
            "Epoch [7/10], Step [290/63], Loss: 0.0151\n",
            "Epoch [7/10], Step [300/63], Loss: 0.0196\n",
            "Epoch [7/10], Step [310/63], Loss: 0.0046\n",
            "Epoch [7/10], Step [320/63], Loss: 0.0003\n",
            "Epoch [7/10], Step [330/63], Loss: 0.3720\n",
            "Epoch [7/10], Step [340/63], Loss: 0.0001\n",
            "Epoch [7/10], Step [350/63], Loss: 0.0034\n",
            "Epoch [7/10], Step [360/63], Loss: 0.0033\n",
            "Epoch [7/10], Step [370/63], Loss: 0.0393\n",
            "Epoch [7/10], Step [380/63], Loss: 0.0111\n",
            "Epoch [7/10], Step [390/63], Loss: 0.0002\n",
            "Epoch [7/10], Step [400/63], Loss: 0.4886\n",
            "Epoch [7/10], Step [410/63], Loss: 0.0047\n",
            "Epoch [7/10], Step [420/63], Loss: 0.0971\n",
            "Epoch [7/10], Step [430/63], Loss: 0.0417\n",
            "Epoch [7/10], Step [440/63], Loss: 0.0013\n",
            "Epoch [7/10], Step [450/63], Loss: 0.0036\n",
            "Epoch [7/10], Step [460/63], Loss: 0.0905\n",
            "Epoch [7/10], Step [470/63], Loss: 0.0455\n",
            "Epoch [7/10], Step [480/63], Loss: 0.0000\n",
            "Epoch [7/10], Step [490/63], Loss: 0.1072\n",
            "Epoch [7/10], Step [500/63], Loss: 0.0046\n",
            "Epoch [8/10], Step [10/63], Loss: 0.0544\n",
            "Epoch [8/10], Step [20/63], Loss: 0.0008\n",
            "Epoch [8/10], Step [30/63], Loss: 0.0002\n",
            "Epoch [8/10], Step [40/63], Loss: 0.0000\n",
            "Epoch [8/10], Step [50/63], Loss: 0.0000\n",
            "Epoch [8/10], Step [60/63], Loss: 0.4421\n",
            "Epoch [8/10], Step [70/63], Loss: 0.0070\n",
            "Epoch [8/10], Step [80/63], Loss: 0.0001\n",
            "Epoch [8/10], Step [90/63], Loss: 0.0257\n",
            "Epoch [8/10], Step [100/63], Loss: 0.1874\n",
            "Epoch [8/10], Step [110/63], Loss: 0.0000\n",
            "Epoch [8/10], Step [120/63], Loss: 0.0253\n",
            "Epoch [8/10], Step [130/63], Loss: 0.0223\n",
            "Epoch [8/10], Step [140/63], Loss: 0.0343\n",
            "Epoch [8/10], Step [150/63], Loss: 0.0045\n",
            "Epoch [8/10], Step [160/63], Loss: 0.0065\n",
            "Epoch [8/10], Step [170/63], Loss: 0.0142\n",
            "Epoch [8/10], Step [180/63], Loss: 0.0598\n",
            "Epoch [8/10], Step [190/63], Loss: 0.0324\n",
            "Epoch [8/10], Step [200/63], Loss: 0.0001\n",
            "Epoch [8/10], Step [210/63], Loss: 0.0028\n",
            "Epoch [8/10], Step [220/63], Loss: 0.0025\n",
            "Epoch [8/10], Step [230/63], Loss: 0.0044\n",
            "Epoch [8/10], Step [240/63], Loss: 0.0004\n",
            "Epoch [8/10], Step [250/63], Loss: 0.0000\n",
            "Epoch [8/10], Step [260/63], Loss: 0.0010\n",
            "Epoch [8/10], Step [270/63], Loss: 0.0007\n",
            "Epoch [8/10], Step [280/63], Loss: 0.0963\n",
            "Epoch [8/10], Step [290/63], Loss: 0.0120\n",
            "Epoch [8/10], Step [300/63], Loss: 0.0371\n",
            "Epoch [8/10], Step [310/63], Loss: 0.0056\n",
            "Epoch [8/10], Step [320/63], Loss: 0.0001\n",
            "Epoch [8/10], Step [330/63], Loss: 0.9038\n",
            "Epoch [8/10], Step [340/63], Loss: 0.0004\n",
            "Epoch [8/10], Step [350/63], Loss: 0.0034\n",
            "Epoch [8/10], Step [360/63], Loss: 0.0008\n",
            "Epoch [8/10], Step [370/63], Loss: 0.0124\n",
            "Epoch [8/10], Step [380/63], Loss: 0.0103\n",
            "Epoch [8/10], Step [390/63], Loss: 0.0002\n",
            "Epoch [8/10], Step [400/63], Loss: 0.1120\n",
            "Epoch [8/10], Step [410/63], Loss: 0.0005\n",
            "Epoch [8/10], Step [420/63], Loss: 0.1162\n",
            "Epoch [8/10], Step [430/63], Loss: 0.0553\n",
            "Epoch [8/10], Step [440/63], Loss: 0.0008\n",
            "Epoch [8/10], Step [450/63], Loss: 0.0024\n",
            "Epoch [8/10], Step [460/63], Loss: 0.1075\n",
            "Epoch [8/10], Step [470/63], Loss: 0.0217\n",
            "Epoch [8/10], Step [480/63], Loss: 0.0000\n",
            "Epoch [8/10], Step [490/63], Loss: 0.1120\n",
            "Epoch [8/10], Step [500/63], Loss: 0.0030\n",
            "Epoch [9/10], Step [10/63], Loss: 0.0785\n",
            "Epoch [9/10], Step [20/63], Loss: 0.0001\n",
            "Epoch [9/10], Step [30/63], Loss: 0.0002\n",
            "Epoch [9/10], Step [40/63], Loss: 0.0000\n",
            "Epoch [9/10], Step [50/63], Loss: 0.0000\n",
            "Epoch [9/10], Step [60/63], Loss: 0.5131\n",
            "Epoch [9/10], Step [70/63], Loss: 0.0122\n",
            "Epoch [9/10], Step [80/63], Loss: 0.0000\n",
            "Epoch [9/10], Step [90/63], Loss: 0.0123\n",
            "Epoch [9/10], Step [100/63], Loss: 0.0553\n",
            "Epoch [9/10], Step [110/63], Loss: 0.0000\n",
            "Epoch [9/10], Step [120/63], Loss: 0.0401\n",
            "Epoch [9/10], Step [130/63], Loss: 0.0018\n",
            "Epoch [9/10], Step [140/63], Loss: 0.0578\n",
            "Epoch [9/10], Step [150/63], Loss: 0.0009\n",
            "Epoch [9/10], Step [160/63], Loss: 0.0049\n",
            "Epoch [9/10], Step [170/63], Loss: 0.0102\n",
            "Epoch [9/10], Step [180/63], Loss: 0.0815\n",
            "Epoch [9/10], Step [190/63], Loss: 0.0233\n",
            "Epoch [9/10], Step [200/63], Loss: 0.0001\n",
            "Epoch [9/10], Step [210/63], Loss: 0.0015\n",
            "Epoch [9/10], Step [220/63], Loss: 0.0141\n",
            "Epoch [9/10], Step [230/63], Loss: 0.0119\n",
            "Epoch [9/10], Step [240/63], Loss: 0.0005\n",
            "Epoch [9/10], Step [250/63], Loss: 0.0000\n",
            "Epoch [9/10], Step [260/63], Loss: 0.0002\n",
            "Epoch [9/10], Step [270/63], Loss: 0.0021\n",
            "Epoch [9/10], Step [280/63], Loss: 0.0338\n",
            "Epoch [9/10], Step [290/63], Loss: 0.0070\n",
            "Epoch [9/10], Step [300/63], Loss: 0.0132\n",
            "Epoch [9/10], Step [310/63], Loss: 0.0024\n",
            "Epoch [9/10], Step [320/63], Loss: 0.0003\n",
            "Epoch [9/10], Step [330/63], Loss: 0.0436\n",
            "Epoch [9/10], Step [340/63], Loss: 0.0000\n",
            "Epoch [9/10], Step [350/63], Loss: 0.0034\n",
            "Epoch [9/10], Step [360/63], Loss: 0.0020\n",
            "Epoch [9/10], Step [370/63], Loss: 0.0184\n",
            "Epoch [9/10], Step [380/63], Loss: 0.0192\n",
            "Epoch [9/10], Step [390/63], Loss: 0.0004\n",
            "Epoch [9/10], Step [400/63], Loss: 0.4831\n",
            "Epoch [9/10], Step [410/63], Loss: 0.0030\n",
            "Epoch [9/10], Step [420/63], Loss: 0.0999\n",
            "Epoch [9/10], Step [430/63], Loss: 0.0264\n",
            "Epoch [9/10], Step [440/63], Loss: 0.0004\n",
            "Epoch [9/10], Step [450/63], Loss: 0.0010\n",
            "Epoch [9/10], Step [460/63], Loss: 0.0651\n",
            "Epoch [9/10], Step [470/63], Loss: 0.0132\n",
            "Epoch [9/10], Step [480/63], Loss: 0.0000\n",
            "Epoch [9/10], Step [490/63], Loss: 0.0203\n",
            "Epoch [9/10], Step [500/63], Loss: 0.0021\n",
            "Epoch [10/10], Step [10/63], Loss: 0.1389\n",
            "Epoch [10/10], Step [20/63], Loss: 0.0000\n",
            "Epoch [10/10], Step [30/63], Loss: 0.0005\n",
            "Epoch [10/10], Step [40/63], Loss: 0.0000\n",
            "Epoch [10/10], Step [50/63], Loss: 0.0000\n",
            "Epoch [10/10], Step [60/63], Loss: 0.7790\n",
            "Epoch [10/10], Step [70/63], Loss: 0.1046\n",
            "Epoch [10/10], Step [80/63], Loss: 0.0000\n",
            "Epoch [10/10], Step [90/63], Loss: 0.0115\n",
            "Epoch [10/10], Step [100/63], Loss: 0.0333\n",
            "Epoch [10/10], Step [110/63], Loss: 0.0000\n",
            "Epoch [10/10], Step [120/63], Loss: 0.0127\n",
            "Epoch [10/10], Step [130/63], Loss: 0.0950\n",
            "Epoch [10/10], Step [140/63], Loss: 0.0230\n",
            "Epoch [10/10], Step [150/63], Loss: 0.0125\n",
            "Epoch [10/10], Step [160/63], Loss: 0.0043\n",
            "Epoch [10/10], Step [170/63], Loss: 0.0076\n",
            "Epoch [10/10], Step [180/63], Loss: 0.0496\n",
            "Epoch [10/10], Step [190/63], Loss: 0.0680\n",
            "Epoch [10/10], Step [200/63], Loss: 0.0001\n",
            "Epoch [10/10], Step [210/63], Loss: 0.0041\n",
            "Epoch [10/10], Step [220/63], Loss: 0.0275\n",
            "Epoch [10/10], Step [230/63], Loss: 0.0195\n",
            "Epoch [10/10], Step [240/63], Loss: 0.0011\n",
            "Epoch [10/10], Step [250/63], Loss: 0.0000\n",
            "Epoch [10/10], Step [260/63], Loss: 0.0004\n",
            "Epoch [10/10], Step [270/63], Loss: 0.0006\n",
            "Epoch [10/10], Step [280/63], Loss: 0.0402\n",
            "Epoch [10/10], Step [290/63], Loss: 0.0009\n",
            "Epoch [10/10], Step [300/63], Loss: 0.0524\n",
            "Epoch [10/10], Step [310/63], Loss: 0.0011\n",
            "Epoch [10/10], Step [320/63], Loss: 0.0003\n",
            "Epoch [10/10], Step [330/63], Loss: 0.0366\n",
            "Epoch [10/10], Step [340/63], Loss: 0.0000\n",
            "Epoch [10/10], Step [350/63], Loss: 0.0045\n",
            "Epoch [10/10], Step [360/63], Loss: 0.0018\n",
            "Epoch [10/10], Step [370/63], Loss: 0.0042\n",
            "Epoch [10/10], Step [380/63], Loss: 0.0179\n",
            "Epoch [10/10], Step [390/63], Loss: 0.0002\n",
            "Epoch [10/10], Step [400/63], Loss: 0.0425\n",
            "Epoch [10/10], Step [410/63], Loss: 0.0005\n",
            "Epoch [10/10], Step [420/63], Loss: 0.1307\n",
            "Epoch [10/10], Step [430/63], Loss: 0.0314\n",
            "Epoch [10/10], Step [440/63], Loss: 0.0002\n",
            "Epoch [10/10], Step [450/63], Loss: 0.0007\n",
            "Epoch [10/10], Step [460/63], Loss: 0.1659\n",
            "Epoch [10/10], Step [470/63], Loss: 0.0090\n",
            "Epoch [10/10], Step [480/63], Loss: 0.0000\n",
            "Epoch [10/10], Step [490/63], Loss: 0.0031\n",
            "Epoch [10/10], Step [500/63], Loss: 0.0012\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    for i in range(train_features.shape[0]):\n",
        "        # Forward pass\n",
        "        outputs = classifier(train_features[i].unsqueeze(0))\n",
        "        loss = criterion(outputs, train_labels[i].unsqueeze(0))\n",
        "\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7ukh_nuYcvk"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Zeng7zcYdCS",
        "outputId": "413aeee5-fd79-4b18-d387-de76e0dc87b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 95.60%\n"
          ]
        }
      ],
      "source": [
        "classifier.eval()  \n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i in range(test_features.shape[0]):\n",
        "        outputs = classifier(test_features[i].unsqueeze(0))\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += test_labels[i].unsqueeze(0).size(0)\n",
        "        correct += (predicted == test_labels[i].unsqueeze(0)).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Test Accuracy: {accuracy:.2f}%')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
